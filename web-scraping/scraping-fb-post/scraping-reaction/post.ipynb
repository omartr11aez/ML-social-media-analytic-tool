{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b668a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.by import By\n",
    "with open('facebook_credentials.txt') as file:\n",
    "    EMAIL = file.readline().split('\"')[1]\n",
    "    PASSWORD = file.readline().split('\"')[1]\n",
    "\n",
    "def idd(item):\n",
    "    for i in range(lenOfPage):\n",
    "        ids = item.find_all(class_=\"_7k7 storyStream _2v9s\")\n",
    "    return ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd861e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_post_text(item):\n",
    "    actualPosts = item.find_all(class_=\"_5rgt _5nk5 _5wnf\")\n",
    "    actualPostss = item.find_all(class_=\"c\")\n",
    "    text = \"\"\n",
    "    if actualPosts:\n",
    "        for posts in actualPosts:\n",
    "            paragraphs =posts.find_all(class_='_x7p _3nc8')\n",
    "            text = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                text += paragraphs[index].text\n",
    "                time.sleep(2)\n",
    "    else:\n",
    "        for posts in actualPostss:\n",
    "            paragraphs =posts.find_all(class_='_x7p _3nc8')\n",
    "            text = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                text += paragraphs[index].text\n",
    "                time.sleep(2)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c2b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_name_reaction(item):\n",
    "    actualname = item.find_all(class_=\"darkTouch _1aj5 l\")\n",
    "    names = []  # fix 1: initialize names as a list\n",
    "    for post in actualname:  # fix 2: change posts to post\n",
    "        name = post.get('href')\n",
    "        names.append(name)  # fix 3: append name to the list names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3b0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_post_id(item):\n",
    "    postIds = item.find_all(class_=\"ib cc _1aj4\")\n",
    "    post_id = \"\"\n",
    "    for postId in postIds:\n",
    "        post_id = postId.find(class_='darkTouch _1aj5 l').get('href')\n",
    "    return post_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e15d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_time(item):\n",
    "    postPictures = item.find_all(class_=\"_52jc _5qc4 _78cz _24u0 _36xo\")\n",
    "    postPicturess = item.find_all(class_=\"x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv xo1l8bm\")\n",
    "    times = \"\"\n",
    "    if postPictures :\n",
    "        for postPicture in postPictures:\n",
    "            paragraphs =postPicture.find_all('a')\n",
    "            times = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                times += paragraphs[index].text\n",
    "    else:\n",
    "        for postPicture in postPicturess:\n",
    "            paragraphs =postPicture.find_all('span')\n",
    "            times = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                times += paragraphs[index].text\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2fa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_reaction(item):\n",
    "    actualreacts = item.find_all(class_=\"scrollAreaColumn\")\n",
    "    reaction = \"\"\n",
    "    if actualreacts:\n",
    "        for posts in actualreacts:\n",
    "            paragraphs =posts.find_all(attrs={'aria-label': True})\n",
    "            reaction = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                reaction += paragraphs[index].get('aria-label')\n",
    "    return reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4456f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_comments(item):\n",
    "    postComments = item.findAll(\"div\", {\"class\": \"_2b04\"})\n",
    "    comments = []\n",
    "    \n",
    "    for comment in postComments:\n",
    "        if comment.find(class_=\"_2b05\") is None:\n",
    "            continue\n",
    "\n",
    "        commenter = comment.find('a').text\n",
    "        comment_text = comment.find(class_='_x7p _3nc8')\n",
    "\n",
    "        if comment_text is not None:\n",
    "            for posts in comment_text:\n",
    "                paragraphs =posts.find_all(class_='_x7l')\n",
    "                text = \"\"\n",
    "                for index in range(0, len(paragraphs)):\n",
    "                    text += paragraphs[index].text\n",
    "                commenterid = comment.find('a').get('href')\n",
    "                comments.append([commenter, text, commenterid])\n",
    "                time.sleep(5)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b83c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_reactions(item):\n",
    "    actualreacts = item.find_all(class_=\"_10tn\")\n",
    "    reaction = \"\"\n",
    "    for reacts in actualreacts:\n",
    "        reaction =reacts.find(class_='_5p-9 _5p-l').get('aria-label')\n",
    "    return reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30202056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_htmls(bs_data):\n",
    "\n",
    "    #Add to check\n",
    "    with open('./bs.html',\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(bs_data.prettify()))\n",
    "\n",
    "    k = bs_data.find_all(\"div\", { \"id\" : \"rootcontainer\" })\n",
    "    postBigDict = list()\n",
    "    for item in k:\n",
    "        postDict = dict()\n",
    "        #postDict['Post'] = _extract_post_text(item)\n",
    "        #postDict['Comments'] = _extract_comments(item)\n",
    "\n",
    "        postDict['Name_reaction'] = _extract_name_reaction(item)\n",
    "         #postDict['PostId'] = _extract_post_id(item)\n",
    "        # postDict['Image'] = _extract_image(item)\n",
    "        # postDict['Shares'] = _extract_shares(item)\n",
    "        postDict['Post'] = _extract_post_text(item)\n",
    "        postDict['Comments'] = _extract_comments(item)\n",
    "        postDict['Time'] = _extract_time(item)\n",
    "\n",
    "        #Add to check\n",
    "        postBigDict.append(postDict)\n",
    "        with open('./postBigDict.json','w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(postBigDict, ensure_ascii=False).encode('utf-8').decode())\n",
    "\n",
    "        return postBigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419bd869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_html(bs_data):\n",
    "\n",
    "    #Add to check\n",
    "    with open('./bs.html',\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(bs_data.prettify()))\n",
    "\n",
    "    k = bs_data.find_all(\"div\", { \"id\" : \"rootcontainer\" })\n",
    "    postBigDict = list()\n",
    "    for item in k:\n",
    "        postDict = dict()\n",
    "        #postDict['Post'] = _extract_post_text(item)\n",
    "        #postDict['Comments'] = _extract_comments(item)\n",
    "\n",
    "        postDict['Name_reaction'] = _extract_name_reaction(item)\n",
    "         #postDict['PostId'] = _extract_post_id(item)\n",
    "        postDict['Reaction'] = _extract_reaction(item)\n",
    "        # postDict['Shares'] = _extract_shares(item)\n",
    "       \n",
    "\n",
    "        #Add to check\n",
    "        postBigDict.append(postDict)\n",
    "        with open('./postBigDicts.json','w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(postBigDict, ensure_ascii=False).encode('utf-8').decode())\n",
    "\n",
    "        return postBigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218bf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _login(browser, email, password):\n",
    "    browser.get(\"http://facebook.com\")\n",
    "    browser.maximize_window()\n",
    "    browser.find_element(\"name\", \"email\").send_keys(email)\n",
    "    browser.find_element(\"name\", \"pass\").send_keys(password)\n",
    "    browser.find_element(\"name\", \"login\").click()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be4b3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_needed_scrolls(browser, infinite_scroll, numOfPost):\n",
    "    if infinite_scroll:\n",
    "        lenOfPage = browser.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\n",
    "        )\n",
    "    else:\n",
    "        # roughly 8 post per scroll kindaOf\n",
    "        lenOfPage = int(numOfPost / 8)\n",
    "    print(\"Number Of Scrolls Needed \" + str(lenOfPage))\n",
    "    return lenOfPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ddcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scroll(browser, infinite_scroll, lenOfPage):\n",
    "    lastCount = -1\n",
    "    match = False\n",
    "\n",
    "    while not match:\n",
    "        if infinite_scroll:\n",
    "            lastCount = lenOfPage\n",
    "        else:\n",
    "            lastCount += 1\n",
    "\n",
    "        # wait for the browser to load, this time can be changed slightly ~3 seconds with no difference, but 5 seems\n",
    "        # to be stable enough\n",
    "        time.sleep(5)\n",
    "\n",
    "        if infinite_scroll:\n",
    "            lenOfPage = browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "        else:\n",
    "            browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "\n",
    "        if lastCount == lenOfPage:\n",
    "            match = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e8862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec6dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tra(browser):    \n",
    "    tran = \"//div[@class='_x7p _3nc8']//a[text()='See Translation']\"\n",
    "    trans = browser.find_elements(By.XPATH, tran)\n",
    "    for transs in trans:\n",
    "        action = webdriver.common.action_chains.ActionChains(browser)\n",
    "        try:\n",
    "            # move to where the un collapse on is\n",
    "            action.move_to_element_with_offset(transs, 5, 5)\n",
    "            action.perform()\n",
    "            transs.click()\n",
    "        #    time.sleep(20)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "        # do nothing right here\n",
    "            pass \n",
    "def getBack(browser):\n",
    "    if not browser.current_url.endswith('actualPosts'):\n",
    "        print('redirected!!!')\n",
    "        browser.back()\n",
    "        print('got back!!!')\n",
    "        \n",
    "def replied (browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = \"//a[text()=' replied']\"\n",
    "        unCollapseCommentsButtons = browser.find_elements(By.XPATH, unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "def com2(browser):             \n",
    " #second set comment ranking to show all comments\n",
    "        rankDropdowns = browser.find_elements_by_class_name('_2pln') #select boxes who have rank dropdowns\n",
    "        rankXPath = '//div[contains(concat(\" \", @class, \" \"), \"uiContextualLayerPositioner\") and not(contains(concat(\" \", @class, \" \"), \"hidden_elem\"))]//div/ul/li/a[@class=\"_54nc\"]/span/span/div[@data-ordering=\"RANKED_UNFILTERED\"]'\n",
    "        for rankDropdown in rankDropdowns:\n",
    "            #click to open the filter modal\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element_with_offset(rankDropdown, 5, 5)\n",
    "                action.perform()\n",
    "                rankDropdown.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # if modal is opened filter comments\n",
    "            ranked_unfiltered = browser.find_elements_by_xpath(rankXPath) # RANKED_UNFILTERED => (All Comments)\n",
    "            if len(ranked_unfiltered) > 0:\n",
    "                try:\n",
    "                    ranked_unfiltered[0].click()\n",
    "                except:\n",
    "                    pass   \n",
    "def com3 (browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = \"//a[@class='touchable primary']\"\n",
    "        unCollapseCommentsButtons = browser.find_elements(By.XPATH, unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass    \n",
    "\n",
    "\n",
    "def react(browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        reactt = \"//a[@class='_45m8']\"\n",
    "        reactts = browser.find_elements(By.XPATH, reactt)\n",
    "        for unCollapseComment in reactts:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(10)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "            \n",
    "def rand_proxy():\n",
    "    proxy = random.choice(ips)\n",
    "    return(proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbe8121",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (1857969723.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 57\u001b[1;36m\u001b[0m\n\u001b[1;33m    time.sleep(10)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "def op(browser): \n",
    "    \n",
    "    posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "    for post in posts:\n",
    "        # Open the post\n",
    "        #click to open the filter modal\n",
    "        action = webdriver.common.action_chains.ActionChains(browser)\n",
    "        try:\n",
    "                action.move_to_element_with_offset(post, 5, 5)\n",
    "                action.perform()\n",
    "                post.click()\n",
    "                time.sleep(10)\n",
    "                tra(browser)\n",
    "                time.sleep(10)\n",
    "                browser.back()\n",
    "        except:\n",
    "                pass\n",
    "        \n",
    "        # Get the comments\n",
    "        # comments = driver.find_elements_by_xpath(\"//div[@class='_3l3x']\")\n",
    "        # Iterate over the comments\n",
    "         #for comment in comments:\n",
    "            # Scrape the comment\n",
    "           #  text = comment.find_element_by_class_name('_19_u').text\n",
    "            # Do something with the comment\n",
    "            # print(text)\n",
    "#         Go back to the post list\n",
    "   #     browser.back()\n",
    "    \n",
    "    \n",
    "def od(browser,posts):\n",
    "   # while True:\n",
    "        # Get a list of all the posts on the page\n",
    "     #   posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "    try:\n",
    "        # Open each post one by one\n",
    "        for i in range(len(posts)):\n",
    "            posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "            time.sleep(10)\n",
    "            if posts[i] in posts:\n",
    "                posts[i].click()\n",
    "            \n",
    "            # Do something with the post, such as scrape its text or comments\n",
    "        \n",
    "            # Navigate back to the main Facebook page\n",
    "                time.sleep(10)\n",
    "                tran = \"//div[@class='_x7p _3nc8']//a[text()='See Translation']\"\n",
    "                trans = browser.find_elements(By.XPATH, tran)\n",
    "                for transs in trans:\n",
    "                    action = webdriver.common.action_chains.ActionChains(browser)\n",
    "                    try:\n",
    "                    # move to where the un collapse on is\n",
    "                        action.move_to_element_with_offset(transs, 5, 5)\n",
    "                        action.perform()\n",
    "                        transs.click()\n",
    "        #    time.sleep(20)\n",
    "                time.sleep(10)\n",
    "                source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "                bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "                postBigDict = _extract_html(bs_data)\n",
    "                postBigDict += postBigDict\n",
    "                   # except:\n",
    "        # do nothing right here\n",
    "                     #   pass\n",
    "          #  time.sleep(10)\n",
    "                browser.back()\n",
    "                time.sleep(10)\n",
    "    except:\n",
    "        print(\"end this\")\n",
    "    # Check if there are more posts on the page\n",
    "        try:\n",
    "            next_button = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "       #     time.sleep(10)\n",
    "            next_button.click()\n",
    "        except:\n",
    "        # If there are no more posts, break out of the loop\n",
    "           print(\"end\")\n",
    "    return postBigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f54ad611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page, numOfPost, infinite_scroll=False, scrape_comment=False):\n",
    "    global postBigDict\n",
    "\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "   # option.add_argument(\"--headless\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    })\n",
    "    \n",
    "    \n",
    " #  \n",
    "  # proxy= rand_proxy()\n",
    "  #  option.add_argument(f'--proxy-server={proxy}')\n",
    "#\n",
    "    # chromedriver should be in the same folder as file\n",
    "    browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n",
    "    _login(browser, EMAIL, PASSWORD)\n",
    "    browser.get(page)\n",
    "    lenOfPage = _count_needed_scrolls(browser, infinite_scroll, numOfPost)\n",
    "      #  tra(browser)\n",
    "      #   print(\"back tra\")\n",
    "      #  for i in range (3):\n",
    "       # com(browser)\n",
    "      #   com3(browser)\n",
    "      #  tra(browser)\n",
    "       # getBack(browser)\n",
    "       # react(browser)\n",
    "    num_scrolls_to_skip = 368\n",
    "\n",
    "# Scroll down the page by the height of the viewport times the number of times to skip\n",
    "    scroll_height = browser.execute_script(\"return window.innerHeight;\")\n",
    "    for i in range(num_scrolls_to_skip):\n",
    "        browser.execute_script(\"window.scrollBy(0, {});\".format(scroll_height))\n",
    "        time.sleep(5)\n",
    "        \n",
    "    posts = list()\n",
    "    for i in range(1):\n",
    "    # Scroll down to the bottom of the page to load more posts\n",
    "      #  browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Wait for the new posts to load\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Extract the new set of post data from the container element\n",
    "        new_posts = browser.find_elements(By.XPATH,\"//div[@class='_34qc _3hxn _3myz _4b45']//a\")\n",
    "    \n",
    "    # Append the new posts to the list of posts\n",
    "        posts += new_posts  \n",
    "    print(len(posts))\n",
    "        # Open each post one by one\n",
    "    postBigDicts = list()\n",
    "    reactions_dict_list = list()\n",
    "    links_list = list()\n",
    "    for i in range(460,len(posts)):\n",
    "            posts = browser.find_elements(By.XPATH,\"//div[@class='_34qc _3hxn _3myz _4b45']//a\")\n",
    "            time.sleep(10)\n",
    "            if posts[i] in posts:\n",
    "                posts[i].click()\n",
    "                time.sleep(5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")                \n",
    "                time.sleep(5)\n",
    "                react(browser)                \n",
    "                time.sleep(5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                for i in range(10):\n",
    "                    com3 (browser)\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                    time.sleep(5)\n",
    "                source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "                bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "                postBigDict1 = _extract_html(bs_data)\n",
    "                postBigDicts =postBigDicts + postBigDict1\n",
    "                print('po')\n",
    "                browser.back()\n",
    "\n",
    "                try:\n",
    "                    current_link = browser.current_url\n",
    "    # add the current link to the list\n",
    "                    links_list.append(current_link)\n",
    "                    \n",
    "                except:\n",
    "        # If there are no more posts, break out of the loop\n",
    "                    print(\"link\")\n",
    "                browser.back()\n",
    "    # Check if there are more posts on the page\n",
    "   # time.sleep(10)\n",
    "    # click on all the comments to scrape them all!\n",
    "    # TODO: need to add more support for additional second level comments\n",
    "    # TODO: ie. comment of a comment\n",
    "    \n",
    "      #  if scrape_comment:\n",
    "       #      com(browser)\n",
    "   # _scroll(browser, infinite_scroll, lenOfPage)\n",
    "    # Now that the page is fully scrolled, grab the source code.\n",
    "        \n",
    "  #  time.sleep(10)\n",
    "    postBigDict = []\n",
    "\n",
    "    # iterate through both lists and create a new dictionary for each iteration\n",
    "    for i in range(len(postBigDicts)):\n",
    "        postBigDict1 = {'PostId': links_list[i], 'Name_reaction': postBigDicts[i]['Name_reaction'], 'Reaction': postBigDicts[i]['Reaction']}\n",
    "        postBigDict.append(postBigDict1)\n",
    "    browser.close()\n",
    "\n",
    "    return postBigDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb43ae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yassi\\AppData\\Local\\Temp\\ipykernel_3856\\2492897965.py:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Scrolls Needed 12\n",
      "470\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Facebook Page Scraper\")\n",
    "    required_parser = parser.add_argument_group(\"required arguments\")\n",
    "    required_parser.add_argument('-page', '-p', help=\"The Facebook Public Page you want to scrape\", required=True)\n",
    "    required_parser.add_argument('-len', '-l', help=\"Number of Posts you want to scrape\", type=int, required=True)\n",
    "    optional_parser = parser.add_argument_group(\"optional arguments\")\n",
    "    optional_parser.add_argument('-infinite', '-i',\n",
    "                                 help=\"Scroll until the end of the page (1 = infinite) (Default is 0)\", type=int,\n",
    "                                 default=0)\n",
    "    optional_parser.add_argument('-usage', '-u', help=\"What to do with the data: \"\n",
    "                                                      \"Print on Screen (PS), \"\n",
    "                                                      \"Write to Text File (WT) (Default is WT)\", default=\"CSV\")\n",
    "\n",
    "    optional_parser.add_argument('-comments', '-c', help=\"Scrape ALL Comments of Posts (y/n) (Default is n). When \"\n",
    "                                                         \"enabled for pages where there are a lot of comments it can \"\n",
    "                                                         \"take a while\", default=\"No\")\n",
    "    args = argparse.Namespace(page='https://m.facebook.com/profile.php?id=100064769104263', len=100 , infinite =0 ,usage=\"CSV\", comments = \"no\")\n",
    "\n",
    "    infinite = False\n",
    "    if args.infinite == 1:\n",
    "        infinite = True\n",
    "\n",
    "    scrape_comment = False\n",
    "    if args.comments == 'y':\n",
    "        scrape_comment = True\n",
    "\n",
    "    postBigDict = extract(page=args.page, numOfPost=args.len, infinite_scroll=infinite, scrape_comment=scrape_comment)\n",
    "\n",
    "\n",
    "    #TODO: rewrite parser\n",
    "    if args.usage == \"WT\":\n",
    "        with open('output.txt', 'w') as file:\n",
    "            for post in postBigDict:\n",
    "                file.write(json.dumps(post))  # use json load to recover\n",
    "\n",
    "    elif args.usage == \"CSV\":\n",
    "        if os.path.exists(\"data.csv\"):\n",
    "            with open('data.csv', 'a',newline='', encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                for post in postBigDict:\n",
    "                    writer.writerow([post['PostId'],post['Name_reaction'], post['Reaction']])#, post['Link'],post['Image'], post['Comments'], post['Shares']])\n",
    "              #writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Reaction']])\n",
    "        else:\n",
    "            with open('data.csv', 'w',newline='', encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "           #writer.writerow(['Post', 'Link', 'Image', 'Comments', 'Reaction'])\n",
    "                writer.writerow(['PostId','Name_reaction','Reaction'])#, 'Link', 'Image', 'Comments', 'Shares'])\n",
    "                for post in postBigDict:\n",
    "                    writer.writerow([ post['PostId'],post['Name_reaction'], post['Reaction']])#, post['Link'],post['Image'], post['Comments'], post['Shares']])\n",
    "              #writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Reaction']])\n",
    "    else:\n",
    "        for post in postBigDict:\n",
    "            print(post)\n",
    "\n",
    "    print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78bc36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Facebook Page Scraper\")\n",
    "    required_parser = parser.add_argument_group(\"required arguments\")\n",
    "    required_parser.add_argument('-page', '-p',default=\"https://www.facebook.com/TunisieTelecom\", help=\"The Facebook Public Page you want to scrape\", required=True)\n",
    "    required_parser.add_argument('-len', '-l',default=20, help=\"Number of Posts you want to scrape\", type=int, required=True)\n",
    "    optional_parser = parser.add_argument_group(\"optional arguments\")\n",
    "    optional_parser.add_argument('-infinite', '-i',\n",
    "                                 help=\"Scroll until the end of the page (1 = infinite) (Default is 0)\", type=int,\n",
    "                                 default=0)\n",
    "    optional_parser.add_argument('-usage', '-u', help=\"What to do with the data: \"\n",
    "                                                      \"Print on Screen (PS), \"\n",
    "                                                      \"Write to Text File (WT) (Default is WT)\", default=\"CSV\")\n",
    "\n",
    "    optional_parser.add_argument('-comments', '-c', help=\"Scrape ALL Comments of Posts (y/n) (Default is n). When \"\n",
    "                                                         \"enabled for pages where there are a lot of comments it can \"\n",
    "                                                         \"take a while\", default=\"No\")\n",
    "    \n",
    "    return parser. parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "print(sys. argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d353b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser. parse_args('dir1 dir2 foobar'. split())\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49664a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if csvfile.tell() == 0:\n",
    "        writer.writerow(['header1', 'header2', 'header3'])\n",
    "    else:\n",
    "        # move the file pointer to the end of the last row\n",
    "        csvfile.seek(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed45b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def extract(page, numOfPost, infinite_scroll=False, scrape_comment=False):\n",
    "    global postBigDict\n",
    "\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "   # option.add_argument(\"--headless\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    })\n",
    "    \n",
    "    \n",
    " #  \n",
    "  # proxy= rand_proxy()\n",
    "  #  option.add_argument(f'--proxy-server={proxy}')\n",
    "#\n",
    "    # chromedriver should be in the same folder as file\n",
    "    browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n",
    "    _login(browser, EMAIL, PASSWORD)\n",
    "    browser.get(page)\n",
    "    lenOfPage = _count_needed_scrolls(browser, infinite_scroll, numOfPost)\n",
    "      #  tra(browser)\n",
    "      #   print(\"back tra\")\n",
    "      #  for i in range (3):\n",
    "       # com(browser)\n",
    "      #   com3(browser)\n",
    "      #  tra(browser)\n",
    "       # getBack(browser)\n",
    "       # react(browser)\n",
    "    posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "    for i in range(1):\n",
    "    # Scroll down to the bottom of the page to load more posts\n",
    "      #  browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Wait for the new posts to load\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Extract the new set of post data from the container element\n",
    "        new_posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "    \n",
    "    # Append the new posts to the list of posts\n",
    "        posts += new_posts  \n",
    "    try:\n",
    "        # Open each post one by one\n",
    "        postBigDict = list()\n",
    "        for i in range(len(posts)):\n",
    "            posts = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "            time.sleep(10)\n",
    "            if posts[i] in posts:\n",
    "                posts[i].click()\n",
    "            \n",
    "            # Do something with the post, such as scrape its text or comments\n",
    "                time.sleep(10)\n",
    "            # Do something with the post, such as scrape its text or comments\n",
    "                react(browser)\n",
    "                try:\n",
    "                    source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "                    bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "                    postBigDict1 = _extract_htmls(bs_data)\n",
    "                    postBigDict =postBigDict + postBigDict1\n",
    "                except:\n",
    "                        print('error bs')\n",
    "            # Navigate back to the main Facebook page\n",
    "                time.sleep(10)\n",
    "            # Navigate back to the main Facebook page\n",
    "                tran = \"//div[@class='_x7p _3nc8']//a[text()='See Translation']\"\n",
    "                trans = browser.find_elements(By.XPATH, tran)\n",
    "                for transs in trans:\n",
    "                    action = webdriver.common.action_chains.ActionChains(browser)\n",
    "                    try:\n",
    "                    # move to where the un collapse on is\n",
    "                        action.move_to_element_with_offset(transs, 5, 5)\n",
    "                        action.perform()\n",
    "                        transs.click()\n",
    "                    except:\n",
    "        # do nothing right here\n",
    "                        pass\n",
    "        #    time.sleep(20)\n",
    "                \n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "                    bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "                    postBigDict2 = _extract_html(bs_data)\n",
    "                    postBigDict =postBigDict + postBigDict1\n",
    "                except:\n",
    "                    print('error bs')\n",
    "        # do nothing right here\n",
    "                     #   pass\n",
    "          #  time.sleep(10)\n",
    "                browser.back()\n",
    "                time.sleep(10)\n",
    "    except:\n",
    "        print(\"end this\")\n",
    "    # Check if there are more posts on the page\n",
    "        try:\n",
    "            next_button = browser.find_elements(By.XPATH,\"//a[@class='_5msj']\")\n",
    "       #     time.sleep(10)\n",
    "            next_button.click()\n",
    "            time.sleep(10)\n",
    "            source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "            bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "            postBigDict = _extract_html(bs_data)\n",
    "            \n",
    "        except:\n",
    "        # If there are no more posts, break out of the loop\n",
    "           print(\"end\")\n",
    "   # time.sleep(10)\n",
    "    # click on all the comments to scrape them all!\n",
    "    # TODO: need to add more support for additional second level comments\n",
    "    # TODO: ie. comment of a comment\n",
    "    \n",
    "      #  if scrape_comment:\n",
    "       #      com(browser)\n",
    "   # _scroll(browser, infinite_scroll, lenOfPage)\n",
    "    # Now that the page is fully scrolled, grab the source code.\n",
    "        \n",
    "  #  time.sleep(10)\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "    return postBigDict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef50c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
