{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b668a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.by import By\n",
    "with open('facebook_credentials.txt') as file:\n",
    "    EMAIL = file.readline().split('\"')[1]\n",
    "    PASSWORD = file.readline().split('\"')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c2b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_name_reaction(item):\n",
    "    actualname = item.find_all(class_=\"darkTouch _1aj5 l\")\n",
    "    names = []  # fix 1: initialize names as a list\n",
    "    for post in actualname:  # fix 2: change posts to post\n",
    "        name = post.get('href')\n",
    "        names.append(name)  # fix 3: append name to the list names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3b0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_post_id(item):\n",
    "    postIds = item.find_all(class_=\"ib cc _1aj4\")\n",
    "    post_id = \"\"\n",
    "    for postId in postIds:\n",
    "        post_id = postId.find(class_='darkTouch _1aj5 l').get('href')\n",
    "    return post_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2fa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_reaction(item):\n",
    "    actualreacts = item.find_all(class_=\"scrollAreaColumn\")\n",
    "    reaction = \"\"\n",
    "    if actualreacts:\n",
    "        for posts in actualreacts:\n",
    "            paragraphs =posts.find_all(attrs={'aria-label': True})\n",
    "            reaction = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                reaction += paragraphs[index].get('aria-label')\n",
    "    return reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419bd869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_html(bs_data):\n",
    "\n",
    "    #Add to check\n",
    "    with open('./bs.html',\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(bs_data.prettify()))\n",
    "\n",
    "    k = bs_data.find_all(\"div\", { \"id\" : \"rootcontainer\" })\n",
    "    postBigDict = list()\n",
    "    for item in k:\n",
    "        postDict = dict()\n",
    "        #postDict['Post'] = _extract_post_text(item)\n",
    "        #postDict['Comments'] = _extract_comments(item)\n",
    "\n",
    "        postDict['Name_reaction'] = _extract_name_reaction(item)\n",
    "         #postDict['PostId'] = _extract_post_id(item)\n",
    "        postDict['Reaction'] = _extract_reaction(item)\n",
    "        # postDict['Shares'] = _extract_shares(item)\n",
    "       \n",
    "\n",
    "        #Add to check\n",
    "        postBigDict.append(postDict)\n",
    "        with open('./postBigDicts.json','w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(postBigDict, ensure_ascii=False).encode('utf-8').decode())\n",
    "\n",
    "        return postBigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218bf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _login(browser, email, password):\n",
    "    browser.get(\"http://facebook.com\")\n",
    "    browser.maximize_window()\n",
    "    browser.find_element(\"name\", \"email\").send_keys(email)\n",
    "    browser.find_element(\"name\", \"pass\").send_keys(password)\n",
    "    browser.find_element(\"name\", \"login\").click()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be4b3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_needed_scrolls(browser, infinite_scroll, numOfPost):\n",
    "    if infinite_scroll:\n",
    "        lenOfPage = browser.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\n",
    "        )\n",
    "    else:\n",
    "        # roughly 8 post per scroll kindaOf\n",
    "        lenOfPage = int(numOfPost / 8)\n",
    "    print(\"Number Of Scrolls Needed \" + str(lenOfPage))\n",
    "    return lenOfPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ddcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scroll(browser, infinite_scroll, lenOfPage):\n",
    "    lastCount = -1\n",
    "    match = False\n",
    "\n",
    "    while not match:\n",
    "        if infinite_scroll:\n",
    "            lastCount = lenOfPage\n",
    "        else:\n",
    "            lastCount += 1\n",
    "\n",
    "        # wait for the browser to load, this time can be changed slightly ~3 seconds with no difference, but 5 seems\n",
    "        # to be stable enough\n",
    "        time.sleep(5)\n",
    "\n",
    "        if infinite_scroll:\n",
    "            lenOfPage = browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "        else:\n",
    "            browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "\n",
    "        if lastCount == lenOfPage:\n",
    "            match = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec6dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tra(browser):    \n",
    "    tran = \"//div[@class='_x7p _3nc8']//a[text()='See Translation']\"\n",
    "    trans = browser.find_elements(By.XPATH, tran)\n",
    "    for transs in trans:\n",
    "        action = webdriver.common.action_chains.ActionChains(browser)\n",
    "        try:\n",
    "            # move to where the un collapse on is\n",
    "            action.move_to_element_with_offset(transs, 5, 5)\n",
    "            action.perform()\n",
    "            transs.click()\n",
    "        #    time.sleep(20)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "        # do nothing right here\n",
    "            pass \n",
    "def getBack(browser):\n",
    "    if not browser.current_url.endswith('actualPosts'):\n",
    "        print('redirected!!!')\n",
    "        browser.back()\n",
    "        print('got back!!!')\n",
    "        \n",
    "def replied (browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = \"//a[text()=' replied']\"\n",
    "        unCollapseCommentsButtons = browser.find_elements(By.XPATH, unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "def com2(browser):             \n",
    " #second set comment ranking to show all comments\n",
    "        rankDropdowns = browser.find_elements_by_class_name('_2pln') #select boxes who have rank dropdowns\n",
    "        rankXPath = '//div[contains(concat(\" \", @class, \" \"), \"uiContextualLayerPositioner\") and not(contains(concat(\" \", @class, \" \"), \"hidden_elem\"))]//div/ul/li/a[@class=\"_54nc\"]/span/span/div[@data-ordering=\"RANKED_UNFILTERED\"]'\n",
    "        for rankDropdown in rankDropdowns:\n",
    "            #click to open the filter modal\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element_with_offset(rankDropdown, 5, 5)\n",
    "                action.perform()\n",
    "                rankDropdown.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # if modal is opened filter comments\n",
    "            ranked_unfiltered = browser.find_elements_by_xpath(rankXPath) # RANKED_UNFILTERED => (All Comments)\n",
    "            if len(ranked_unfiltered) > 0:\n",
    "                try:\n",
    "                    ranked_unfiltered[0].click()\n",
    "                except:\n",
    "                    pass   \n",
    "def com3 (browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = \"//a[@class='touchable primary']\"\n",
    "        unCollapseCommentsButtons = browser.find_elements(By.XPATH, unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass    \n",
    "\n",
    "\n",
    "def react(browser):        \n",
    "#first uncollapse collapsed comments\n",
    "        reactt = \"//a[@class='_45m8']\"\n",
    "        reactts = browser.find_elements(By.XPATH, reactt)\n",
    "        for unCollapseComment in reactts:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "                time.sleep(10)\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "            \n",
    "def rand_proxy():\n",
    "    proxy = random.choice(ips)\n",
    "    return(proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f54ad611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page, numOfPost, infinite_scroll=False, scrape_comment=False):\n",
    "    global postBigDict\n",
    "\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "   # option.add_argument(\"--headless\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    })\n",
    "    \n",
    "    \n",
    " #  \n",
    "  # proxy= rand_proxy()\n",
    "  #  option.add_argument(f'--proxy-server={proxy}')\n",
    "#\n",
    "    # chromedriver should be in the same folder as file\n",
    "    browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n",
    "    _login(browser, EMAIL, PASSWORD)\n",
    "    browser.get(page)\n",
    "    lenOfPage = _count_needed_scrolls(browser, infinite_scroll, numOfPost)\n",
    "      \n",
    "    num_scrolls_to_skip = 368\n",
    "\n",
    "# Scroll down the page by the height of the viewport times the number of times to skip\n",
    "    scroll_height = browser.execute_script(\"return window.innerHeight;\")\n",
    "    for i in range(num_scrolls_to_skip):\n",
    "        browser.execute_script(\"window.scrollBy(0, {});\".format(scroll_height))\n",
    "        time.sleep(5)\n",
    "        \n",
    "    posts = list()\n",
    "    for i in range(1):\n",
    "    \n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Extract the new set of post data from the container element\n",
    "        new_posts = browser.find_elements(By.XPATH,\"//div[@class='_34qc _3hxn _3myz _4b45']//a\")\n",
    "    \n",
    "    # Append the new posts to the list of posts\n",
    "        posts += new_posts  \n",
    "    print(len(posts))\n",
    "        # Open each post one by one\n",
    "    postBigDicts = list()\n",
    "    reactions_dict_list = list()\n",
    "    links_list = list()\n",
    "    for i in range(460,len(posts)):\n",
    "            posts = browser.find_elements(By.XPATH,\"//div[@class='_34qc _3hxn _3myz _4b45']//a\")\n",
    "            time.sleep(10)\n",
    "            if posts[i] in posts:\n",
    "                posts[i].click()\n",
    "                time.sleep(5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")                \n",
    "                time.sleep(5)\n",
    "                react(browser)                \n",
    "                time.sleep(5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                for i in range(10):\n",
    "                    com3 (browser)\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "                    time.sleep(5)\n",
    "                source_data = browser.page_source\n",
    "   # time.sleep(10)\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "                bs_data = bs(source_data, 'html.parser')\n",
    "  #  time.sleep(10)\n",
    "                postBigDict1 = _extract_html(bs_data)\n",
    "                postBigDicts =postBigDicts + postBigDict1\n",
    "                print('po')\n",
    "                browser.back()\n",
    "\n",
    "                try:\n",
    "                    current_link = browser.current_url\n",
    "    # add the current link to the list\n",
    "                    links_list.append(current_link)\n",
    "                    \n",
    "                except:\n",
    "        # If there are no more posts, break out of the loop\n",
    "                    print(\"link\")\n",
    "                browser.back()\n",
    "    \n",
    "    postBigDict = []\n",
    "\n",
    "    # iterate through both lists and create a new dictionary for each iteration\n",
    "    for i in range(len(postBigDicts)):\n",
    "        postBigDict1 = {'PostId': links_list[i], 'Name_reaction': postBigDicts[i]['Name_reaction'], 'Reaction': postBigDicts[i]['Reaction']}\n",
    "        postBigDict.append(postBigDict1)\n",
    "    browser.close()\n",
    "\n",
    "    return postBigDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb43ae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yassi\\AppData\\Local\\Temp\\ipykernel_3856\\2492897965.py:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Scrolls Needed 12\n",
      "470\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Facebook Page Scraper\")\n",
    "    required_parser = parser.add_argument_group(\"required arguments\")\n",
    "    required_parser.add_argument('-page', '-p', help=\"The Facebook Public Page you want to scrape\", required=True)\n",
    "    required_parser.add_argument('-len', '-l', help=\"Number of Posts you want to scrape\", type=int, required=True)\n",
    "    optional_parser = parser.add_argument_group(\"optional arguments\")\n",
    "    optional_parser.add_argument('-infinite', '-i',\n",
    "                                 help=\"Scroll until the end of the page (1 = infinite) (Default is 0)\", type=int,\n",
    "                                 default=0)\n",
    "    optional_parser.add_argument('-usage', '-u', help=\"What to do with the data: \"\n",
    "                                                      \"Print on Screen (PS), \"\n",
    "                                                      \"Write to Text File (WT) (Default is WT)\", default=\"CSV\")\n",
    "\n",
    "    optional_parser.add_argument('-comments', '-c', help=\"Scrape ALL Comments of Posts (y/n) (Default is n). When \"\n",
    "                                                         \"enabled for pages where there are a lot of comments it can \"\n",
    "                                                         \"take a while\", default=\"No\")\n",
    "    args = argparse.Namespace(page='https://m.facebook.com/profile.php?id=100064769104263', len=100 , infinite =0 ,usage=\"CSV\", comments = \"no\")\n",
    "\n",
    "    infinite = False\n",
    "    if args.infinite == 1:\n",
    "        infinite = True\n",
    "\n",
    "    scrape_comment = False\n",
    "    if args.comments == 'y':\n",
    "        scrape_comment = True\n",
    "\n",
    "    postBigDict = extract(page=args.page, numOfPost=args.len, infinite_scroll=infinite, scrape_comment=scrape_comment)\n",
    "\n",
    "\n",
    "    #TODO: rewrite parser\n",
    "    if args.usage == \"WT\":\n",
    "        with open('output.txt', 'w') as file:\n",
    "            for post in postBigDict:\n",
    "                file.write(json.dumps(post))  # use json load to recover\n",
    "\n",
    "    elif args.usage == \"CSV\":\n",
    "        if os.path.exists(\"data.csv\"):\n",
    "            with open('data.csv', 'a',newline='', encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                for post in postBigDict:\n",
    "                    writer.writerow([post['PostId'],post['Name_reaction'], post['Reaction']])\n",
    "              \n",
    "        else:\n",
    "            with open('data.csv', 'w',newline='', encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "           \n",
    "                writer.writerow(['PostId','Name_reaction','Reaction'])\n",
    "                for post in postBigDict:\n",
    "                    writer.writerow([ post['PostId'],post['Name_reaction'], post['Reaction']])\n",
    "              \n",
    "    else:\n",
    "        for post in postBigDict:\n",
    "            print(post)\n",
    "\n",
    "    print(\"Finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
